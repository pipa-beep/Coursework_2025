{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Импорт библиотек**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Загрузка датасетов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = pd.read_parquet(\"interactions.parquet\")\n",
    "tracks = pd.read_parquet(\"tracks.parquet\")\n",
    "catalog_names = pd.read_parquet(\"catalog_names.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обрежу самыый большой датасет для скорости**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.iloc[0:100_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100000 entries, 0 to 45\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count   Dtype         \n",
      "---  ------      --------------   -----         \n",
      " 0   user_id     100000 non-null  int32         \n",
      " 1   track_id    100000 non-null  int32         \n",
      " 2   track_seq   100000 non-null  int16         \n",
      " 3   started_at  100000 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int16(1), int32(2)\n",
      "memory usage: 2.5 MB\n"
     ]
    }
   ],
   "source": [
    "interactions.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сортирую track_seq и групирую по user_id**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = interactions.sort_values(by=['user_id', 'track_seq'])\n",
    "user_sequences = interactions.groupby('user_id')['track_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>track_seq</th>\n",
       "      <th>started_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>99262</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>589498</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>590262</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>590303</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>590692</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-07-22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  track_id  track_seq started_at\n",
       "0        0     99262          1 2022-07-17\n",
       "1        0    589498          2 2022-07-19\n",
       "2        0    590262          3 2022-07-21\n",
       "3        0    590303          4 2022-07-22\n",
       "4        0    590692          5 2022-07-22"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>track_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[99262, 589498, 590262, 590303, 590692, 590803...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[24417, 108208, 108209, 592642, 628687, 733449...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[264937, 672689, 4321285, 5335351, 5658525, 58...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[6006252, 21642261, 21642265, 24692821, 259952...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[966, 4094, 9760, 9769, 18392, 19042, 21184, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                           track_id\n",
       "0        0  [99262, 589498, 590262, 590303, 590692, 590803...\n",
       "1        1  [24417, 108208, 108209, 592642, 628687, 733449...\n",
       "2        2  [264937, 672689, 4321285, 5335351, 5658525, 58...\n",
       "3        3  [6006252, 21642261, 21642265, 24692821, 259952...\n",
       "4        4  [966, 4094, 9760, 9769, 18392, 19042, 21184, 2..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_sequences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем все уникальные треки в **all_track** и создаем **MASK_TOKEN** для маскировки токенов в последовательности. **PAD** токены нужны просто как плейсхолдеры для того что бы каждая последовательность была одинаковой длинны. \n",
    "\n",
    "Дальше мы создаем словарь где ключем будет **track_id** а **value** порядковым номером в словаре. Так же создаем инверсивный словарь для обратного получения **track_id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tracks = interactions['track_id'].unique().tolist()\n",
    "MASK_TOKEN = \"<MASK>\"\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "\n",
    "vocab = {PAD_TOKEN: 0, MASK_TOKEN: 1}\n",
    "for track in all_tracks:\n",
    "    if track not in vocab:\n",
    "        vocab[track] = len(vocab)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sequences: список списков осстоящих из track_id пользователей \n",
    "vocab: track_id -> index\n",
    "mask_prob: вероятность замаскировать токен\n",
    "\n",
    "_ _ len _ _ :  возвращает длину **sequences**. #это нужно для  PyTorch датасетов\n",
    "\n",
    "_ _ getitem _ _ : сама функция используется внутри PyTorch (потому что это map-styled dataset). \n",
    "1) Преобразуем все **id**  треков в sequence в индексы, а если трека нет в vocab то в **PAD** токен.\n",
    "2) Потом если длина **sequence** больше максимальной задданной длины, мы укарачиваем sequence. В обратном случае мы дополняем ее **PAD** токенами.\n",
    "3) Дальше мы создаем **target** последовательность и начальную последовательность, в которой мы маскируем с шансом **mask_prob** токены в **MASK_TOKEN**.\n",
    "4) Делаем из каждой последовательности тензор для **PyTorch**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4RecDataset(Dataset):\n",
    "    def __init__(self, sequences, vocab, max_seq_len=50, mask_prob = 0.7):\n",
    "        self.sequences = sequences\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences[idx]\n",
    "        seq_idx = [self.vocab.get(token, self.vocab[PAD_TOKEN]) for token in seq]\n",
    "        \n",
    "        if len(seq_idx) >= self.max_seq_len:\n",
    "            seq_idx = seq_idx[-self.max_seq_len:]\n",
    "        else:\n",
    "            seq_idx = [self.vocab[PAD_TOKEN]] * (self.max_seq_len - len(seq_idx)) + seq_idx\n",
    "        \n",
    "        input_seq = list(seq_idx)\n",
    "        target_seq = list(seq_idx)\n",
    "        \n",
    "        for i in range(len(input_seq)):\n",
    "            if input_seq[i] != self.vocab[PAD_TOKEN] and random.random() < self.mask_prob:\n",
    "                input_seq[i] = self.vocab[MASK_TOKEN]\n",
    "        \n",
    "        input_seq = torch.tensor(input_seq, dtype=torch.long)\n",
    "        target_seq = torch.tensor(target_seq, dtype=torch.long)\n",
    "        \n",
    "        return input_seq, target_seq\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "sequences = user_sequences['track_id'].tolist()\n",
    "train_sequences, val_sequences = train_test_split(sequences, test_size = 0.2, random_state = 42)\n",
    "train_dataset = BERT4RecDataset(train_sequences, vocab, max_seq_len=50, mask_prob=0.7)\n",
    "val_dataset = BERT4RecDataset(val_sequences, vocab, max_seq_len=50, mask_prob=0.7)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы описываем модель, сначала идет конструктор со строкой super... которая вызывает конструктор nn.Module (это нужно для коректной работы объявления следущих переменых)\n",
    "\n",
    "1) дальше идет функция forward которая принимает на вход тензор x (batch_size, seq_len).\n",
    "2) positions это просто тензор с нумерацией мест по типу ((0, 1),(0, 1)) и.т.д. Потом мы прогоняем positions через nn.Embedding присваивая по началу рандомные значения для векторов(это изменится на моменте оптимизатора и бекпропагейшн).\n",
    "3) Далее мы складываем позиционные эмбединги с эмбедингами токенов и в последствии нормируем, что бы значения были в равных пределах (от 0 до 1). И далее делаем дропаут для добавления случайного шума (это помогает модели не переобучаться)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT4Rec(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=128, num_heads=8, num_layers=4, max_seq_len=50, dropout=0.1): \n",
    "        \n",
    "        super(BERT4Rec, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.position_embeddings = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        token_emb = self.token_embeddings(x)\n",
    "        \n",
    "        positions = torch.arange(seq_len, device=x.device).unsqueeze(0).expand(batch_size, seq_len)\n",
    "        pos_emb = self.position_embeddings(positions)\n",
    "        \n",
    "        x = token_emb + pos_emb\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "       \n",
    "        x = x.transpose(0, 1) #[seq_len, batch_size, embed_dim]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.transpose(0, 1)  #[B, seq_len, embed_dim]\n",
    "        \n",
    "        logits = self.output_layer(x)  # [B, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# Создаем модель\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BERT4Rec(vocab_size=vocab_size, embed_dim=128, num_heads=4, num_layers=2, max_seq_len=50, dropout=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall_ndcg(model, dataloader, k=10, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_hits = 0  # для recall\n",
    "    total_ndcg = 0.0  # для NDCG\n",
    "    total_count = 0  # общее число позиций (без PAD)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            logits = model(inputs)  # [B, seq_len, vocab_size]\n",
    "            topk = logits.topk(k, dim=-1).indices  # [B, seq_len, k]\n",
    "            \n",
    "            batch_size, seq_len = targets.size()\n",
    "            for i in range(batch_size):\n",
    "                for j in range(seq_len):\n",
    "                    true_token = targets[i, j].item()\n",
    "                    if true_token == 0:\n",
    "                        continue\n",
    "                    total_count += 1\n",
    "                    topk_tokens = topk[i, j].tolist()\n",
    "                    if true_token in topk_tokens:\n",
    "                        total_hits += 1\n",
    "                        rank = topk_tokens.index(true_token) + 1\n",
    "                        total_ndcg += 1 / np.log2(rank + 1)\n",
    "    \n",
    "    recall = total_hits / total_count if total_count > 0 else 0\n",
    "    ndcg = total_ndcg / total_count if total_count > 0 else 0\n",
    "    return recall, ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 11.0097\n",
      "Validation Recall@10: 0.0022, NDCG@10: 0.0011\n",
      "Epoch 2/30, Loss: 10.3269\n",
      "Validation Recall@10: 0.0102, NDCG@10: 0.0059\n",
      "Epoch 3/30, Loss: 9.7263\n",
      "Validation Recall@10: 0.0285, NDCG@10: 0.0194\n",
      "Epoch 4/30, Loss: 9.2861\n",
      "Validation Recall@10: 0.0369, NDCG@10: 0.0280\n",
      "Epoch 5/30, Loss: 8.9433\n",
      "Validation Recall@10: 0.0535, NDCG@10: 0.0400\n",
      "Epoch 6/30, Loss: 8.6516\n",
      "Validation Recall@10: 0.0642, NDCG@10: 0.0501\n",
      "Epoch 7/30, Loss: 8.3700\n",
      "Validation Recall@10: 0.0770, NDCG@10: 0.0602\n",
      "Epoch 8/30, Loss: 8.0994\n",
      "Validation Recall@10: 0.0879, NDCG@10: 0.0683\n",
      "Epoch 9/30, Loss: 7.8117\n",
      "Validation Recall@10: 0.1026, NDCG@10: 0.0795\n",
      "Epoch 10/30, Loss: 7.5210\n",
      "Validation Recall@10: 0.1147, NDCG@10: 0.0903\n",
      "Epoch 11/30, Loss: 7.2242\n",
      "Validation Recall@10: 0.1243, NDCG@10: 0.1002\n",
      "Epoch 12/30, Loss: 6.9213\n",
      "Validation Recall@10: 0.1385, NDCG@10: 0.1130\n",
      "Epoch 13/30, Loss: 6.6211\n",
      "Validation Recall@10: 0.1528, NDCG@10: 0.1244\n",
      "Epoch 14/30, Loss: 6.3218\n",
      "Validation Recall@10: 0.1553, NDCG@10: 0.1267\n",
      "Epoch 15/30, Loss: 6.0117\n",
      "Validation Recall@10: 0.1679, NDCG@10: 0.1411\n",
      "Epoch 16/30, Loss: 5.7224\n",
      "Validation Recall@10: 0.1687, NDCG@10: 0.1437\n",
      "Epoch 17/30, Loss: 5.4164\n",
      "Validation Recall@10: 0.1712, NDCG@10: 0.1471\n",
      "Epoch 18/30, Loss: 5.1530\n",
      "Validation Recall@10: 0.1798, NDCG@10: 0.1563\n",
      "Epoch 19/30, Loss: 4.8507\n",
      "Validation Recall@10: 0.1850, NDCG@10: 0.1628\n",
      "Epoch 20/30, Loss: 4.5880\n",
      "Validation Recall@10: 0.1915, NDCG@10: 0.1689\n",
      "Epoch 21/30, Loss: 4.3093\n",
      "Validation Recall@10: 0.1999, NDCG@10: 0.1776\n",
      "Epoch 22/30, Loss: 4.0622\n",
      "Validation Recall@10: 0.2088, NDCG@10: 0.1855\n",
      "Epoch 23/30, Loss: 3.8180\n",
      "Validation Recall@10: 0.2083, NDCG@10: 0.1850\n",
      "Epoch 24/30, Loss: 3.5544\n",
      "Validation Recall@10: 0.2143, NDCG@10: 0.1896\n",
      "Epoch 25/30, Loss: 3.3556\n",
      "Validation Recall@10: 0.2264, NDCG@10: 0.2030\n",
      "Epoch 26/30, Loss: 3.1358\n",
      "Validation Recall@10: 0.2294, NDCG@10: 0.2056\n",
      "Epoch 27/30, Loss: 2.9144\n",
      "Validation Recall@10: 0.2326, NDCG@10: 0.2095\n",
      "Epoch 28/30, Loss: 2.7620\n",
      "Validation Recall@10: 0.2423, NDCG@10: 0.2215\n",
      "Epoch 29/30, Loss: 2.5374\n",
      "Validation Recall@10: 0.2475, NDCG@10: 0.2248\n",
      "Epoch 30/30, Loss: 2.3896\n",
      "Validation Recall@10: 0.2589, NDCG@10: 0.2364\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab[PAD_TOKEN])\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        outputs = outputs.view(-1, vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    val_recall, val_ndcg = compute_recall_ndcg(model, val_loader, k=10, device=device)\n",
    "    print(f\"Validation Recall@10: {val_recall:.4f}, NDCG@10: {val_ndcg:.4f}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_track_name(track_id, catalog_names_df):\n",
    "    row = catalog_names_df[\n",
    "        (catalog_names_df['id'] == track_id) &\n",
    "        (catalog_names_df['type'] == 'track')\n",
    "    ]\n",
    "    if not row.empty:\n",
    "        return row['name'].values[0]\n",
    "    else:\n",
    "        return \"Unknown Track\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_track_ids_to_names(track_ids, catalog_names_df):\n",
    "    names = []\n",
    "    for track_id in track_ids:\n",
    "        if track_id in [MASK_TOKEN, PAD_TOKEN]:\n",
    "            names.append(track_id)\n",
    "            continue\n",
    "        row = catalog_names_df[\n",
    "            (catalog_names_df['id'] == track_id) & \n",
    "            (catalog_names_df['type'] == 'track')\n",
    "        ]\n",
    "        if not row.empty:\n",
    "            names.append(row['name'].values[0])\n",
    "        else:\n",
    "            names.append(\"Unknown Track\")\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходная последовательность (названия):\n",
      "- Boulevard of Broken Dreams\n",
      "- Rise Up\n",
      "- Long Nights\n",
      "- Night: Part I. Snow\n",
      "- Faded\n",
      "- Are You With Me\n",
      "- Никому не отдам\n",
      "- Stressed Out\n",
      "- Here She Comes Again\n",
      "- The Hills\n",
      "- I Took A Pill In Ibiza\n",
      "- Ты не такой\n",
      "- Грею счастье\n",
      "- Faded\n",
      "- Bludfire\n",
      "- Hymn For The Weekend\n",
      "- California\n",
      "- Boom Boom Boom\n",
      "- Don't Let Me Down\n",
      "- Cheap Thrills\n",
      "- This Girl\n",
      "- Save Me\n",
      "- Feel\n",
      "- You Are The Only One\n",
      "- HandClap\n",
      "- Мегахит\n",
      "- The Ocean\n",
      "- Непохожие\n",
      "- Выпускной (Медлячок)\n",
      "- Кружит\n",
      "- Sing Me to Sleep\n",
      "- Sweet Harmony (feat. Pearl Andersson)\n",
      "- Perfect Strangers\n",
      "- Tuesday\n",
      "- I Never Felt so Right\n",
      "- Wide Awake\n",
      "- Lost on You\n",
      "- Пусть весь мир подождёт\n",
      "- Lordly\n",
      "- Вдвоём\n",
      "- My Way\n",
      "- Rockabye\n",
      "Predicted Track Name: FIND YOUR WAY BACK\n"
     ]
    }
   ],
   "source": [
    "def predict_masked(model, sequence, vocab, max_seq_len=50):\n",
    "    model.eval()\n",
    "    seq_idx = [vocab.get(token, vocab[PAD_TOKEN]) for token in sequence]\n",
    "    if len(seq_idx) >= max_seq_len:\n",
    "        seq_idx = seq_idx[-max_seq_len:]\n",
    "    else:\n",
    "        seq_idx = [vocab[PAD_TOKEN]] * (max_seq_len - len(seq_idx)) + seq_idx\n",
    "    input_seq = torch.tensor(seq_idx, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_seq)\n",
    "\n",
    "    pred_tokens = logits.argmax(dim=-1).squeeze(0).cpu().numpy()\n",
    "    return pred_tokens\n",
    "\n",
    "\n",
    "sample_seq = sequences[12]\n",
    "\n",
    "sample_seq_masked = sample_seq[:-1] + [MASK_TOKEN]\n",
    "predicted = predict_masked(model, sample_seq_masked, vocab)\n",
    "predicted_token_id = predicted[-1]\n",
    "predicted_track_id = inv_vocab[predicted_token_id]\n",
    "track_name = get_track_name(predicted_track_id, catalog_names)\n",
    "track_names = convert_track_ids_to_names(sample_seq, catalog_names)\n",
    "print(\"Initial sequence (names):\")\n",
    "for name in track_names:\n",
    "    print(\"-\", name)\n",
    "print(\"Predicted Track Name:\", track_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
